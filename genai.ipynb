{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMffhx0xITRyvPnFSZB3ucZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sohv/ai_from_scratch/blob/main/genai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WGAN"
      ],
      "metadata": {
        "id": "z6pfJihGUioP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ciW67HNT5Mi"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, LeakyReLU, BatchNormalization, Dropout\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = (x_train.astype(np.float32) - 127.5) / 127.5  # Normalize to [-1,1]\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "\n",
        "# Model parameters\n",
        "latent_dim = 100\n",
        "img_shape = (28, 28, 1)\n",
        "clip_value = 0.01\n",
        "n_critic = 5\n",
        "\n",
        "# Build Generator\n",
        "def build_generator():\n",
        "    model = Sequential([\n",
        "        Dense(7 * 7 * 128, activation='relu', input_dim=latent_dim),\n",
        "        Reshape((7, 7, 128)),\n",
        "        BatchNormalization(),\n",
        "        Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Conv2DTranspose(1, kernel_size=4, strides=1, padding='same', activation='tanh')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Build Critic (Discriminator)\n",
        "def build_critic():\n",
        "    model = Sequential([\n",
        "        Conv2D(64, kernel_size=4, strides=2, padding='same', input_shape=img_shape),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dropout(0.25),\n",
        "        Conv2D(128, kernel_size=4, strides=2, padding='same'),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dropout(0.25),\n",
        "        Flatten(),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create models\n",
        "generator = build_generator()\n",
        "critic = build_critic()\n",
        "critic.compile(loss='mse', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "critic.trainable = False\n",
        "\n",
        "gan_input = tf.keras.Input(shape=(latent_dim,))\n",
        "gan_output = critic(generator(gan_input))\n",
        "gan = Model(gan_input, gan_output)\n",
        "gan.compile(loss='mse', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "# Training function\n",
        "def train_wgan(epochs=10000, batch_size=128, sample_interval=1000):\n",
        "    for epoch in range(epochs):\n",
        "        for _ in range(n_critic):\n",
        "            idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "            real_imgs = x_train[idx]\n",
        "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "            fake_imgs = generator.predict(noise)\n",
        "            d_loss_real = critic.train_on_batch(real_imgs, -np.ones((batch_size, 1)))\n",
        "            d_loss_fake = critic.train_on_batch(fake_imgs, np.ones((batch_size, 1)))\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "            for layer in critic.layers:\n",
        "                for w in layer.trainable_weights:\n",
        "                    w.assign(tf.clip_by_value(w, -clip_value, clip_value))\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        g_loss = gan.train_on_batch(noise, -np.ones((batch_size, 1)))\n",
        "        if epoch % sample_interval == 0:\n",
        "            print(f\"Epoch {epoch}, D Loss: {d_loss}, G Loss: {g_loss}\")\n",
        "            sample_images(epoch)\n",
        "\n",
        "# Function to generate images\n",
        "def sample_images(epoch):\n",
        "    noise = np.random.normal(0, 1, (10, latent_dim))\n",
        "    generated_imgs = generator.predict(noise)\n",
        "    generated_imgs = 0.5 * generated_imgs + 0.5  # Rescale to [0,1]\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    for i in range(10):\n",
        "        plt.subplot(1, 10, i+1)\n",
        "        plt.imshow(generated_imgs[i, :, :, 0], cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Train WGAN\n",
        "train_wgan(epochs=10000, batch_size=128, sample_interval=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DCGAN"
      ],
      "metadata": {
        "id": "TxpXs1GuUmAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, LeakyReLU, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = (x_train.astype(np.float32) - 127.5) / 127.5  # Normalize to [-1,1]\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "\n",
        "# Model parameters\n",
        "latent_dim = 100\n",
        "img_shape = (28, 28, 1)\n",
        "\n",
        "# Build Generator\n",
        "def build_generator():\n",
        "    model = Sequential([\n",
        "        Dense(7 * 7 * 128, activation='relu', input_dim=latent_dim),\n",
        "        Reshape((7, 7, 128)),\n",
        "        BatchNormalization(),\n",
        "        Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Conv2DTranspose(1, kernel_size=4, strides=1, padding='same', activation='tanh')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Build Discriminator\n",
        "def build_discriminator():\n",
        "    model = Sequential([\n",
        "        Conv2D(64, kernel_size=4, strides=2, padding='same', input_shape=img_shape),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Conv2D(128, kernel_size=4, strides=2, padding='same'),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Flatten(),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create models\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "discriminator.trainable = False\n",
        "\n",
        "gan_input = tf.keras.Input(shape=(latent_dim,))\n",
        "gan_output = discriminator(generator(gan_input))\n",
        "gan = tf.keras.Model(gan_input, gan_output)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "# Training function\n",
        "def train_dcgan(epochs=10000, batch_size=128, sample_interval=1000):\n",
        "    for epoch in range(epochs):\n",
        "        idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "        real_imgs = x_train[idx]\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        fake_imgs = generator.predict(noise)\n",
        "        d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((batch_size, 1)))\n",
        "        d_loss_fake = discriminator.train_on_batch(fake_imgs, np.zeros((batch_size, 1)))\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
        "        if epoch % sample_interval == 0:\n",
        "            print(f\"Epoch {epoch}, D Loss: {d_loss[0]}, G Loss: {g_loss}\")\n",
        "            sample_images(epoch)\n",
        "\n",
        "# Function to generate images\n",
        "def sample_images(epoch):\n",
        "    noise = np.random.normal(0, 1, (10, latent_dim))\n",
        "    generated_imgs = generator.predict(noise)\n",
        "    generated_imgs = 0.5 * generated_imgs + 0.5  # Rescale to [0,1]\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    for i in range(10):\n",
        "        plt.subplot(1, 10, i+1)\n",
        "        plt.imshow(generated_imgs[i, :, :, 0], cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Train DCGAN\n",
        "train_dcgan(epochs=10000, batch_size=128, sample_interval=1000)"
      ],
      "metadata": {
        "id": "pgImpCoLT_gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN"
      ],
      "metadata": {
        "id": "c02toTKfUqVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Reshape, LeakyReLU\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = (x_train.astype(np.float32) - 127.5) / 127.5  # Normalize to [-1,1]\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "\n",
        "# Define model parameters\n",
        "latent_dim = 100\n",
        "img_shape = (28, 28, 1)\n",
        "\n",
        "# Build Generator\n",
        "def build_generator():\n",
        "    model = Sequential([\n",
        "        Dense(256, activation=LeakyReLU(alpha=0.2), input_dim=latent_dim),\n",
        "        Dense(512, activation=LeakyReLU(alpha=0.2)),\n",
        "        Dense(1024, activation=LeakyReLU(alpha=0.2)),\n",
        "        Dense(28*28*1, activation='tanh'),\n",
        "        Reshape(img_shape)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Build Discriminator\n",
        "def build_discriminator():\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=img_shape),\n",
        "        Dense(512, activation=LeakyReLU(alpha=0.2)),\n",
        "        Dense(256, activation=LeakyReLU(alpha=0.2)),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile models\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "discriminator.trainable = False\n",
        "\n",
        "gan_input = tf.keras.Input(shape=(latent_dim,))\n",
        "gan_output = discriminator(generator(gan_input))\n",
        "gan = Model(gan_input, gan_output)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "# Training function\n",
        "def train_gan(epochs=10000, batch_size=128, sample_interval=1000):\n",
        "    for epoch in range(epochs):\n",
        "        # Train Discriminator\n",
        "        idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "        real_imgs = x_train[idx]\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        fake_imgs = generator.predict(noise)\n",
        "        d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((batch_size, 1)))\n",
        "        d_loss_fake = discriminator.train_on_batch(fake_imgs, np.zeros((batch_size, 1)))\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # Train Generator\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
        "\n",
        "        # Print progress\n",
        "        if epoch % sample_interval == 0:\n",
        "            print(f\"Epoch {epoch}, D Loss: {d_loss[0]}, G Loss: {g_loss}\")\n",
        "            sample_images(epoch)\n",
        "\n",
        "# Function to generate and save images\n",
        "def sample_images(epoch):\n",
        "    noise = np.random.normal(0, 1, (10, latent_dim))\n",
        "    generated_imgs = generator.predict(noise)\n",
        "    generated_imgs = 0.5 * generated_imgs + 0.5  # Rescale to [0,1]\n",
        "\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    for i in range(10):\n",
        "        plt.subplot(1, 10, i+1)\n",
        "        plt.imshow(generated_imgs[i, :, :, 0], cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Train GAN\n",
        "train_gan(epochs=10000, batch_size=128, sample_interval=1000)"
      ],
      "metadata": {
        "id": "ziLXjwBKUrpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AE"
      ],
      "metadata": {
        "id": "UtLMH9SjUvf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
        "\n",
        "# Define encoding dimension\n",
        "encoding_dim = 32\n",
        "\n",
        "# Encoder\n",
        "input_img = Input(shape=(28, 28, 1))\n",
        "x = Flatten()(input_img)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dense(encoding_dim, activation='relu')(x)\n",
        "encoder = Model(input_img, x, name='encoder')\n",
        "\n",
        "# Decoder\n",
        "decoder_input = Input(shape=(encoding_dim,))\n",
        "x = Dense(128, activation='relu')(decoder_input)\n",
        "x = Dense(28*28, activation='sigmoid')(x)\n",
        "outputs = Reshape((28, 28, 1))(x)\n",
        "decoder = Model(decoder_input, outputs, name='decoder')\n",
        "\n",
        "# Autoencoder model\n",
        "autoencoder_output = decoder(encoder(input_img))\n",
        "autoencoder = Model(input_img, autoencoder_output, name='autoencoder')\n",
        "\n",
        "# Compile and train autoencoder\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "autoencoder.fit(x_train, x_train, epochs=20, batch_size=128, validation_data=(x_test, x_test))\n",
        "\n",
        "# Encode and decode test images\n",
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)\n",
        "\n",
        "# Display original and reconstructed images\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28), cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fb_LDLpPUwpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAE"
      ],
      "metadata": {
        "id": "lJkQa6-2U4vP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda, Flatten, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import mse\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
        "\n",
        "# Define encoding dimension\n",
        "latent_dim = 2\n",
        "\n",
        "# Sampling function\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "# Encoder\n",
        "inputs = Input(shape=(28, 28, 1))\n",
        "x = Flatten()(inputs)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(latent_dim,))\n",
        "x = Dense(256, activation='relu')(decoder_inputs)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dense(28 * 28, activation='sigmoid')(x)\n",
        "outputs = Reshape((28, 28, 1))(x)\n",
        "\n",
        "decoder = Model(decoder_inputs, outputs, name='decoder')\n",
        "\n",
        "# VAE model\n",
        "vae_outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, vae_outputs, name='vae')\n",
        "\n",
        "# VAE loss function\n",
        "reconstruction_loss = mse(Flatten()(inputs), Flatten()(vae_outputs)) * 28 * 28\n",
        "kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
        "vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer='adam')\n",
        "\n",
        "# Train the VAE\n",
        "vae.fit(x_train, epochs=20, batch_size=128, validation_data=(x_test, None))\n",
        "\n",
        "# Generate images from random samples\n",
        "n = 10\n",
        "grid_x = np.linspace(-2, 2, n)\n",
        "grid_y = np.linspace(-2, 2, n)\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i, yi in enumerate(grid_y):\n",
        "    for j, xi in enumerate(grid_x):\n",
        "        z_sample = np.array([[xi, yi]])\n",
        "        x_decoded = decoder.predict(z_sample)\n",
        "        digit = x_decoded[0].reshape(28, 28)\n",
        "        ax = plt.subplot(n, n, i * n + j + 1)\n",
        "        plt.imshow(digit, cmap='gray')\n",
        "        plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yNhrjOBDU6Ws"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}